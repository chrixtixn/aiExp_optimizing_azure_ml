# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This dataset contains data about bank costumers and we seek to predict if a client will subscribe to a bank deposit or not.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best performing model was created by autoML and its Voting Ensemble with XGboost classifier which resulted 0.9088 accuracy.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

In this pipeline architecture, tabular data was first loaded and cleaned to prepare it for modeling. The classification task was carried out using logistic regression, a well-established algorithm for binary and multi-class classification. To optimize the model's performance, hyperparameter tuning was integrated into the pipeline using AzureML's HyperDrive. Two hyperparameters of the logistic regression model were selected for tuning: C, the inverse of regularization strength, and max_iter, which controls the number of training iterations. These were sampled using a random parameter sampler, which allows exploration of the search space without bias, making it a simple yet effective approach when prior knowledge about the best parameter values is limited.

**What are the benefits of the parameter sampler you chose?**

The random sampling method is particularly beneficial because it can discover strong parameter combinations faster than exhaustive methods like grid search, especially in high-dimensional search spaces. It also reduces computational cost by evaluating a random subset instead of all possible combinations.

**What are the benefits of the early stopping policy you chose?**

To further optimize resource usage, a BanditPolicy was applied as an early stopping mechanism. This policy evaluates model performance periodically and terminates runs that are unlikely to outperform the best one found so far, based on a specified slack factor. This strategy helps save time and computational resources by focusing on the most promising configurations during the hyperparameter search. Overall, this setup ensures a scalable, efficient, and automated pipeline for building and tuning a classification model.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
